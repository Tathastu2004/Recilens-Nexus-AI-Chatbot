# main.py - RAG FOCUS
import asyncio
import os
import logging
import uuid
import json
from pathlib import Path
from datetime import datetime
from fastapi import FastAPI, HTTPException, UploadFile, File, Form
from pydantic import BaseModel
from fastapi.responses import StreamingResponse
from fastapi.middleware.cors import CORSMiddleware
from ollama import AsyncClient
from typing import Optional, Dict, Any, List
from dotenv import load_dotenv

# ‚úÖ FIXED: Add proper error handling for optional RAG services
try:
    from services.model_manager import ModelManager
    from services.training_service import IngestionService
    HAS_RAG_SERVICES = True
    print("‚úÖ RAG services imported successfully")
except ImportError as e:
    print(f"‚ö†Ô∏è Warning: Could not import RAG services: {e}")
    print("üìù RAG features will be disabled, but chat will still work")
    HAS_RAG_SERVICES = False
    ModelManager = None
    IngestionService = None

load_dotenv()

LLAMA_URL = os.getenv("LLAMA_URL", "http://127.0.0.1:11434")
NODE_BACKEND_URL = os.getenv("NODE_BACKEND_URL", "http://localhost:3000")

# ‚úÖ FIXED: Initialize RAG services with proper error handling
model_manager = None
ingestion_service = None

if HAS_RAG_SERVICES:
    try:
        print("üîÑ Initializing RAG services...")
        model_manager = ModelManager()
        print("‚úÖ ModelManager created")
        
        ingestion_service = IngestionService(model_manager.get_embedding_model())
        print("‚úÖ IngestionService created")
        
        print("‚úÖ RAG services initialized successfully")
    except Exception as e:
        print(f"‚ùå RAG initialization failed: {e}")
        print(f"‚ùå Error type: {type(e).__name__}")
        import traceback
        print(f"‚ùå Full traceback:")
        traceback.print_exc()
        print("üìù RAG features will be disabled, but chat will still work")
        model_manager = None
        ingestion_service = None  # ‚Üê This is why it's None!

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(title="Nexus AI FastAPI Server", version="1.0.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ‚úÖ PYDANTIC MODELS
class ChatMessage(BaseModel):
    role: str
    content: str
    fileUrl: Optional[str] = None
    fileName: Optional[str] = None
    fileType: Optional[str] = None

class ChatRequest(BaseModel):
    sessionId: str
    message: str
    type: Optional[str] = "text"
    conversation_context: Optional[List[ChatMessage]] = []
    adapter_id: Optional[str] = None

class EnhancedChatRequest(BaseModel):
    sessionId: str
    message: str
    type: Optional[str] = "text"
    conversation_context: Optional[List[ChatMessage]] = []
    adapter_id: Optional[str] = None
    extractedText: Optional[str] = None
    fileUrl: Optional[str] = None
    fileName: Optional[str] = None
    fileType: Optional[str] = None
    textLength: Optional[int] = 0
    documentType: Optional[str] = None
    contextEnabled: Optional[bool] = False
    contextInstruction: Optional[str] = None

class IntentRequest(BaseModel):
    message: str

class IntentResponse(BaseModel):
    intent: str

async def recognize_intent(message: str) -> str:
    message_lower = message.lower()
    if any(word in message_lower for word in ["document", "pdf", "analyze", "file"]): 
        return "document_analysis"
    elif any(word in message_lower for word in ["train", "training", "model", "lora"]): 
        return "training"
    elif any(word in message_lower for word in ["health", "status", "check"]): 
        return "health"
    elif any(word in message_lower for word in ["hello", "hi", "hey"]): 
        return "greeting"
    else: 
        return "general"

@app.get("/health")
async def health_check():
    """Health check with proper RAG service status"""
    try:
        # ‚úÖ Check Ollama connection
        ollama_status = "unknown"
        try:
            client = AsyncClient(host=LLAMA_URL)
            response = await client.list()
            ollama_status = "online" if response else "offline"
        except Exception as e:
            logger.warning(f"Ollama check failed: {e}")
            ollama_status = "offline"

        return {
            "status": "healthy",
            "timestamp": datetime.now().isoformat(),
            "services": {
                "rag_services_available": HAS_RAG_SERVICES,
                "model_manager": model_manager is not None,
                "ingestion_service": ingestion_service is not None,
                "ollama": ollama_status
            },
            "endpoints": {
                "chat": "Available (with or without RAG)",
                "rag_ingestion": "Available" if ingestion_service else "Disabled",
                "document_deletion": "Available" if ingestion_service else "Disabled"
            }
        }
    except Exception as e:
        logger.error(f"Health check failed: {e}")
        raise HTTPException(status_code=500, detail=f"Health check failed: {str(e)}")

@app.get("/rag/health")
async def rag_health_check():
    """Specific RAG health check for adminController"""
    try:
        return {
            "status": "online" if model_manager else "offline",
            "rag_services": HAS_RAG_SERVICES,
            "model_manager": model_manager is not None,
            "ingestion_service": ingestion_service is not None,
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"RAG health check failed: {e}")
        return {
            "status": "offline",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

@app.get("/vector/health")
async def vector_health_check():
    """Vector database health check for adminController"""
    try:
        # Since we don't have a separate vector DB, check if RAG services are available
        return {
            "status": "online" if model_manager else "offline",
            "vector_db": "embedded" if model_manager else "unavailable",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return {
            "status": "offline",
            "error": str(e),
            "timestamp": datetime.now().isoformat()
        }

# ‚úÖ FIXED: ROBUST CHAT ENDPOINT
@app.post("/chat")
async def enhanced_chat_endpoint(request: EnhancedChatRequest):
    try:
        logger.info(f"üí¨ Chat request - Type: {request.type}, Session: {request.sessionId}, RAG enabled: {request.contextEnabled}")
        
        # ‚úÖ SMART RAG DECISION: Only use RAG for document-related queries
        should_use_rag = False
        if request.contextEnabled and model_manager:
            # Check if the query is document-related
            query_lower = request.message.lower()
            document_keywords = [
                'document', 'pdf', 'file', 'uploaded', 'nexus ai', 'assistant', 
                'product overview', 'limitations', 'pricing', 'support contact',
                'maximum file size', 'context window', 'enterprise', 'features'
            ]
            
            # Use RAG only if query contains document-specific terms
            should_use_rag = any(keyword in query_lower for keyword in document_keywords)
            
            if should_use_rag:
                logger.info("üìÑ [RAG] Document-related query detected - using RAG")
            else:
                logger.info("üß† [GENERAL] General knowledge query - using model's knowledge")
        
        # ‚úÖ DOCUMENT PROCESSING WITH OR WITHOUT RAG
        if request.type == "document" and request.extractedText:
            logger.info(f"üìÑ [DOCUMENT] Processing document: {request.fileName} ({len(request.extractedText)} chars)")
            
            document_prompt = f"""You are analyzing a document. Here is the content:

Document Name: {request.fileName}
Document Type: {request.documentType or 'PDF'}
Content Length: {len(request.extractedText)} characters

User Question: {request.message}

Document Content:
{request.extractedText}

Please analyze this document and answer the user's question based on the content above. Be specific and reference details from the document."""

            final_prompt = document_prompt
            
        elif should_use_rag and model_manager:
            # ‚úÖ RAG PIPELINE (only for document-related queries)
            logger.info("üìÑ [RAG] Using RAG for document-specific query")
            try:
                response = model_manager.get_rag_response(request.message, request.conversation_context)
                
                async def stream_rag_response():
                    for i, char in enumerate(response):
                        yield char
                        if i % 3 == 0:
                            await asyncio.sleep(0.01)
                
                return StreamingResponse(stream_rag_response(), media_type="text/plain")
            except Exception as rag_error:
                logger.error(f"‚ùå RAG processing failed: {rag_error}")
                final_prompt = request.message  # Fall back to general knowledge
        
        elif request.contextEnabled and not should_use_rag:
            # ‚úÖ GENERAL KNOWLEDGE QUERY (don't use RAG)
            logger.info("üß† [GENERAL] Using general knowledge for non-document query")
            final_prompt = request.message
            
        elif request.contextEnabled and not model_manager:
            # ‚úÖ RAG requested but not available
            logger.info("üß† [FALLBACK] RAG not available, using general knowledge")
            final_prompt = request.message
            
        elif request.contextInstruction:
            # ‚úÖ USE EXPLICIT CONTEXT INSTRUCTION
            logger.info("üéØ [CONTEXT] Using explicit context instruction")
            final_prompt = f"{request.contextInstruction}\n\nUser question: {request.message}"
            
        else:
            # ‚úÖ REGULAR TEXT PROCESSING
            logger.info("üí¨ [TEXT] Processing regular text message")
            final_prompt = request.message

        # ‚úÖ ADD CONVERSATION CONTEXT
        messages = []
        
        # System message based on query type
        if request.type == "document":
            system_msg = "You are a helpful AI assistant specialized in document analysis. When analyzing documents, be thorough and specific, referencing the actual content."
        elif should_use_rag:
            system_msg = "You are a helpful AI assistant with access to company knowledge base. Use the provided context to give accurate and specific answers about the company's products and services."
        else:
            system_msg = "You are a helpful AI assistant. Use your general knowledge to provide clear, accurate, and helpful responses to any questions."
        
        messages.append({"role": "system", "content": system_msg})

        # Add conversation context if available
        if request.conversation_context and len(request.conversation_context) > 0:
            logger.info(f"üîÑ [CONTEXT] Adding {len(request.conversation_context)} conversation messages")
            for msg in request.conversation_context[-5:]:
                if hasattr(msg, 'role') and hasattr(msg, 'content'):
                    messages.append({"role": msg.role, "content": msg.content})
                elif isinstance(msg, dict):
                    role = msg.get('role', 'user')
                    content = msg.get('content', '')
                    messages.append({"role": role, "content": content})
        
        # Add current message
        messages.append({"role": "user", "content": final_prompt})

        logger.info(f"ü¶ô [OLLAMA] Sending to Ollama with {len(messages)} messages")

        # ‚úÖ OLLAMA CHAT WITH ENHANCED PARAMETERS
        try:
            client = AsyncClient(host=LLAMA_URL)
            
            # Optimized parameters for different content types
            is_document = request.type == "document" or request.extractedText
            
            options = {
                "temperature": 0.3 if (is_document or should_use_rag) else 0.7,
                "top_p": 0.9,
                "repeat_penalty": 1.1,
                "num_predict": 2000 if is_document else 1000
            }
            
            stream = await client.chat(
                model="llama3", 
                messages=messages, 
                stream=True,
                options=options
            )
            
            async def stream_ollama_response():
                try:
                    logger.info("üåä [STREAMING] Starting Ollama response stream")
                    chunk_count = 0
                    total_content = ""
                    
                    async for chunk in stream:
                        if chunk and 'message' in chunk:
                            content = chunk['message'].get('content', '')
                            if content:
                                total_content += content
                                chunk_count += 1
                                yield content
                                
                                if chunk_count % 20 == 0 and is_document:
                                    logger.info(f"üìä [STREAM] Chunk {chunk_count}, Total: {len(total_content)} chars")
                    
                    logger.info(f"‚úÖ [STREAMING] Completed - {chunk_count} chunks, {len(total_content)} characters")
                    
                except Exception as stream_error:
                    logger.error(f"‚ùå [STREAMING] Error: {stream_error}")
                    yield f"\n\n‚ùå Streaming error: {str(stream_error)}"
            
            return StreamingResponse(stream_ollama_response(), media_type="text/plain")
            
        except Exception as ollama_error:
            logger.error(f"‚ùå [OLLAMA] Error: {ollama_error}")
            
            async def error_response():
                error_msg = f"‚ùå AI service temporarily unavailable: {str(ollama_error)}"
                if request.type == "document" and request.fileName:
                    error_msg += f"\n\nI can see you uploaded '{request.fileName}' ({len(request.extractedText or '')} characters), but I'm unable to process it right now."
                error_msg += "\n\nPlease ensure Ollama is running on port 11434 and try again."
                yield error_msg
            
            return StreamingResponse(error_response(), media_type="text/plain")

    except Exception as e:
        logger.error(f"‚ùå [CHAT] Processing failed: {e}")
        
        async def error_response():
            yield f"‚ùå Chat processing failed: {str(e)}. Please try again."
        
        return StreamingResponse(error_response(), media_type="text/plain")

# ‚úÖ RAG-SPECIFIC ENDPOINTS (only if services available)
@app.post("/ingest_data")
async def ingest_data(
    file: UploadFile = File(...),
    doc_id: str = Form(...)
):
    """Ingests a new data sheet (PDF or image) into the RAG system."""
    logger.info(f"üìÑ [INGEST] Ingestion request received - File: {file.filename}, Doc ID: {doc_id}")
    
    if not ingestion_service:
        logger.warning("‚ö†Ô∏è Ingestion requested but RAG service not available")
        # ‚úÖ DON'T raise HTTPException, return success with warning
        return {
            "success": True,  # ‚úÖ Changed to True so backend doesn't think it failed
            "message": f"Document '{file.filename}' received but RAG indexing is not available. Document stored in database only.",
            "status": "stored_no_rag",
            "doc_id": doc_id,
            "filename": file.filename,
            "rag_available": False,
            "rag_indexed": False
        }
    
    file_path = f"data/{doc_id}_{file.filename}"
    os.makedirs("data", exist_ok=True)
    
    try:
        # Save file temporarily
        content = await file.read()
        with open(file_path, "wb") as f:
            f.write(content)
        
        logger.info(f"üìÑ [INGEST] Processing {file.filename} ({len(content)} bytes)")
        
        # Process with ingestion service
        await ingestion_service.ingest_document(file_path, doc_id)
        
        logger.info(f"‚úÖ [INGEST] Successfully ingested {file.filename} with ID: {doc_id}")
        return {
            "success": True, 
            "message": f"Successfully ingested and indexed {file.filename} with ID: {doc_id}",
            "status": "fully_processed",
            "doc_id": doc_id,
            "filename": file.filename,
            "rag_available": True,
            "rag_indexed": True
        }
        
    except Exception as e:
        logger.error(f"‚ùå [INGEST] Ingestion failed: {e}")
        # ‚úÖ Return partial success instead of complete failure
        return {
            "success": True,  # ‚úÖ Document was at least uploaded
            "message": f"Document '{file.filename}' uploaded but RAG processing failed: {str(e)}",
            "status": "upload_success_processing_failed",
            "doc_id": doc_id,
            "filename": file.filename,
            "rag_available": True,
            "rag_indexed": False,
            "error": str(e)
        }
    finally:
        # Clean up temporary file
        if os.path.exists(file_path):
            os.remove(file_path)

@app.delete("/delete_data")
async def delete_data(doc_id: str = Form(...)):
    """Deletes a specific data sheet from the RAG system by its document ID."""
    logger.info(f"üóëÔ∏è [DELETE] Delete request received - Doc ID: {doc_id}")
    
    if not ingestion_service:
        logger.warning("‚ö†Ô∏è Deletion requested but RAG service not available")
        # ‚úÖ DON'T raise HTTPException, return success with warning
        return {
            "success": True,  # ‚úÖ Changed to True so backend doesn't think it failed
            "message": f"Document with ID '{doc_id}' deletion processed. RAG service not available but document can be removed from database.",
            "status": "rag_service_unavailable",
            "doc_id": doc_id,
            "deleted_from_rag": False,
            "rag_available": False
        }
    
    try:
        logger.info(f"üóëÔ∏è [DELETE] Deleting document ID: {doc_id}")
        
        success = ingestion_service.delete_document_by_id(doc_id)
        if success:
            logger.info(f"‚úÖ [DELETE] Successfully deleted document with ID: {doc_id}")
            return {
                "success": True,
                "message": f"Successfully deleted document with ID: {doc_id}",
                "status": "deleted_from_rag",
                "doc_id": doc_id,
                "deleted_from_rag": True,
                "rag_available": True
            }
        else:
            logger.warning(f"‚ö†Ô∏è [DELETE] Document not found in RAG: {doc_id}")
            return {
                "success": True,  # ‚úÖ Still success since it's not in RAG anyway
                "message": f"Document with ID '{doc_id}' not found in RAG index, but database cleanup can proceed.",
                "status": "not_found_in_rag",
                "doc_id": doc_id,
                "deleted_from_rag": False,
                "rag_available": True
            }
            
    except Exception as e:
        logger.error(f"‚ùå [DELETE] Deletion failed: {e}")
        # ‚úÖ Return partial success instead of complete failure
        return {
            "success": True,  # ‚úÖ Database deletion can still proceed
            "message": f"RAG deletion failed for '{doc_id}' but database cleanup can proceed: {str(e)}",
            "status": "rag_deletion_failed",
            "doc_id": doc_id,
            "deleted_from_rag": False,
            "rag_available": True,
            "error": str(e)
        }

@app.post("/intent", response_model=IntentResponse)
async def intent_endpoint(request: IntentRequest):
    try:
        intent = await recognize_intent(request.message)
        logger.info(f"üéØ Intent recognized: {intent} for message: {request.message[:50]}...")
        return IntentResponse(intent=intent)
    except Exception as e:
        logger.error(f"‚ùå Intent recognition failed: {e}")
        return IntentResponse(intent="general")

@app.get("/")
def root():
    return {
        "message": "Nexus AI FastAPI Server - RAG Enabled",
        "status": "running",
        "version": "1.0.0",
        "services": {
            "rag_services": HAS_RAG_SERVICES,
            "model_manager": model_manager is not None,
            "ingestion_service": ingestion_service is not None,
            "ollama_chat": "Always available (if Ollama running)"
        },
        "features": {
            "basic_chat": "‚úÖ Available",
            "document_analysis": "‚úÖ Available (enhanced with RAG if enabled)",
            "rag_ingestion": "‚úÖ Available" if ingestion_service else "‚ùå Service not available",
            "rag_retrieval": "‚úÖ Available" if model_manager else "‚ùå Service not available"
        },
        "endpoints": {
            "/health": "GET (Health check)",
            "/rag/health": "GET (RAG-specific health)",
            "/vector/health": "GET (Vector DB health)",
            "/chat": "POST (Main chat endpoint with document support and optional RAG)",
            "/ingest_data": "POST (Ingest a data sheet)" + (" - Available" if ingestion_service else " - Service not available"),
            "/delete_data": "DELETE (Delete a data sheet)" + (" - Available" if ingestion_service else " - Service not available"),
            "/intent": "POST (Intent recognition)"
        }
    }

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8000)
